{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom time import time\nimport deepspeed\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\n    device_map=\"auto\",\n  torch_dtype=torch.float16,\n  offload_folder=\"offload\",\n  trust_remote_code=True,\n  low_cpu_mem_usage=True)\n    \n\ndef optimize_and_run_mistral(model_path, input_tokens=128, output_tokens=128, model_parallel_size=2):\n    \n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    # Load model in chunks\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n\n    # Cache (stores last output for repeated prompts)\n    cached_output = None\n\n    while True:\n        # Get user prompt\n        prompt = input(\"Enter your prompt (or 'quit' to exit): \")\n\n        if prompt.lower() == \"quit\":\n            break\n\n        # Tokenize input\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n        # Inference\n        start_time = time()\n        if cached_output is not None and prompt == cached_prompt:\n            # Use cached output if prompt matches\n            output_ids = cached_output\n        else:\n            with torch.inference_mode():\n                # Generate model output\n                output_ids = model.generate(input_ids, max_length=output_tokens, use_cache=True)\n\n        # Decode and print output\n        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        print(f\"\\nModel Response: {output_text}\")\n\n        # Calculate and print performance metrics\n        inference_time = time() - start_time\n        throughput = (input_tokens + output_tokens) / inference_time\n        print(f\"\\nInference Latency: {inference_time:.3f} seconds\")\n        print(f\"Throughput: {throughput:.3f} tokens/second\")\n\n        cached_prompt = prompt\n        cached_output = output_ids\n\n#prefer a sharded version or shard before inputting as a technique for efficient cpu/gpu usage\nif __name__ == \"__main__\":\n    model_path = \"alexsherstinsky/Mistral-7B-v0.1-sharded\"  # Replace with your desired model from huggingface\n    model_parallel_size = 2  # Set the model parallel size (number of GPUs) for efficient loading\n    optimize_and_run_mistral(model_path, model_parallel_size=model_parallel_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-07T17:58:16.316599Z","iopub.execute_input":"2024-03-07T17:58:16.317399Z","iopub.status.idle":"2024-03-07T18:00:00.596744Z","shell.execute_reply.started":"2024-03-07T17:58:16.317361Z","shell.execute_reply":"2024-03-07T18:00:00.595930Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba6a80dd22cb43d384179bf2960474b8"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"Enter your prompt (or 'quit' to exit):  heights\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n2024-03-07 17:58:29.694859: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-07 17:58:29.694970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-07 17:58:29.857435: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\nModel Response: heights = [160, 185, 170, 193, 176, 180, 178]\n\n# 1. 평균 구하기\nprint(sum(heights) / len(heights))\n\n# 2. 최대값 구하기\nprint(max(heights))\n\n# 3. 최소값 구하기\nprint(min(heights))\n\n# 4. 중앙값 구하기\n\nInference Latency: 20.859 seconds\nThroughput: 12.273 tokens/second\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your prompt (or 'quit' to exit):  firefighter\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nModel Response: firefighter\n    Joined: Mon Jan 21, 2013 11:00 pm\n\n### Re: The Official \"I'm Going to Law School\" Thread\n\n> *Jay wrote:*\n>\n> > *firefighter wrote:*\n> >\n> > > *Jay wrote:*\n> > >\n> > > > *firefighter wrote:*\n> > > >\n> > > > > *Jay wrote:*\n> > > > >\n> > > > > > *firefighter wrote\n\nInference Latency: 7.675 seconds\nThroughput: 33.355 tokens/second\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your prompt (or 'quit' to exit):  best\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nModel Response: best-selling author, speaker, and coach\n\n# About\n\n## About\n\n### About\n\n##### I’m a best-selling author, speaker, and coach.\n\nI’m a best-selling author, speaker, and coach. I’m also a wife, mother, and grandmother. I’m a former teacher, and I’ve been a student of personal development for over 30 years.\n\nI’ve been a student of personal development for over 30 years. I’ve been a student of personal development for over 30 years. I’\n\nInference Latency: 7.851 seconds\nThroughput: 32.607 tokens/second\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your prompt (or 'quit' to exit):  quit\n"}]}]}